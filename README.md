# CDEvalSumm: Cross-Dataset Evaluation for Summarization

### Descriptions and metrics code for EMNLP2020 findings paper: 
### *[CDEvalSumm: An Empirical Study of Cross-Dataset Evaluation for Neural Summarization Systems]()*
([Yiran Chen*](https://scholar.google.com/citations?hl=zh-CN&user=ZEnShlcAAAAJ), [Pengfei Liu*](https://scholar.google.com/citations?hl=zh-CN&user=oIz_CYEAAAAJ), [Ming Zhong](https://scholar.google.com/citations?hl=zh-CN&user=mnifqeUAAAAJ), [Zi-Yi Dou](https://scholar.google.com/citations?hl=zh-CN&user=RWogNsEAAAAJ), [Danqing Wang](https://scholar.google.com/citations?hl=zh-CN&user=mAo_lUwAAAAJ), [Xipeng Qiu](https://scholar.google.com/citations?hl=zh-CN&user=Pq4Yp_kAAAAJ), [Xuanjing Huang](https://scholar.google.com/citations?hl=zh-CN&user=RGsMgZA4H78C))

## Motivation
Many work evaluate summarization systems on in-domain setting (the model is trained and tested on the same dataset). In this work we try to understand model performance on different perspectives on a cross-dataset setting. The picture blow represents the main motivation (summarization systems get different rankings when evaluated under different measures where abstractive models are red while extractive ones are blue): <br><br>

### Two Research Questions
**Q1**: How do different neural architectures of summarizers influence the cross-dataset generalization performances?<br>
**Q2**: Do different generation ways (extractive and abstractive) of summarizers influence the cross-dataset generalization ability?

<img src="https://github.com/zide05/CompSUM/blob/master/figs/ranking6.png" width="450" height="250">


## Evaluation Systems
  - Extractive summarizers: <img src="https://render.githubusercontent.com/render/math?math=LSTM_{non}, Trans_{non}, Trans_{auto}, BERT_{non}, BERT_{match}">
  <!--$LSTM_{non}$ , $Trans_{non}$ , $Trans_{auto}$ , $BERT_{non}$ , $BERT_{match}$-->
  
  - Abstractive summarizers: <img src="https://render.githubusercontent.com/render/math?math=L2L_{ptr}^{cov}, L2L_{ptr}, L2L, T2T, BE2T, BART">
  <!--$L2L_{ptr}^{cov}$ , $L2L_{ptr}$ , $L2L$ , $T2T$ , $BE2T$ , $BART$-->


<table style="border-collapse:collapse;border-color:#ccc;border-spacing:0;border-style:solid;border-width:1px" class="tg"><thead><tr><th style="background-color:#f0f0f0;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:bottom;word-break:normal" colspan="2">Systems</th><th style="background-color:#f0f0f0;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:bottom;word-break:normal">Paper</th><th style="background-color:#f0f0f0;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal">Bib</th></tr></thead><tbody><tr><td style="background-color:#f9f9f9;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:middle;word-break:normal" rowspan="5">Abs-Sum</td><td style="background-color:#f9f9f9;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal">LSTM_{non}</td><td style="background-color:#f9f9f9;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal"><a href="https://arxiv.org/pdf/1810.12343.pdf" target="_blank" rel="noopener noreferrer">Content Selection in   Deep Learning Models of Summarization</a></td><td style="background-color:#f9f9f9;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal"><a href="https://www.aclweb.org/anthology/D18-1208.bib" target="_blank" rel="noopener noreferrer">Bib</a></td></tr><tr><td style="background-color:#fff;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal">Trans_non</td><td style="background-color:#fff;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal"><a href="https://arxiv.org/pdf/1908.08345.pdf" target="_blank" rel="noopener noreferrer">Text   Summarization  with  Pretrained    Encoders</a></td><td style="background-color:#fff;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal"><a href="https://www.aclweb.org/anthology/D19-1387.bib" target="_blank" rel="noopener noreferrer">Bib</a></td></tr><tr><td style="background-color:#f9f9f9;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal">Trans_{auto}</td><td style="background-color:#f9f9f9;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal"><a href="https://arxiv.org/pdf/1907.03491.pdf" target="_blank" rel="noopener noreferrer">Searching   for Effective  Neural  Extractive    Summarization:  What  works and Whatâ€™s Next</a></td><td style="background-color:#f9f9f9;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal"><a href="https://www.aclweb.org/anthology/P19-1100.bib" target="_blank" rel="noopener noreferrer">Bib</a></td></tr><tr><td style="background-color:#fff;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal">BERT_{non}</td><td style="background-color:#fff;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal"><a href="https://arxiv.org/pdf/1908.08345.pdf" target="_blank" rel="noopener noreferrer">Text   Summarization  with  Pretrained    Encoders</a></td><td style="background-color:#fff;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal"><a href="https://www.aclweb.org/anthology/D19-1387.bib" target="_blank" rel="noopener noreferrer">Bib</a></td></tr><tr><td style="background-color:#f9f9f9;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal">BERT_{match}</td><td style="background-color:#f9f9f9;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal"><a href="https://arxiv.org/pdf/2004.08795.pdf" target="_blank" rel="noopener noreferrer">Extractive   Summarization as Text Matching</a></td><td style="background-color:#f9f9f9;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal"><a href="https://www.aclweb.org/anthology/2020.acl-main.552.bib" target="_blank" rel="noopener noreferrer">Bib</a></td></tr><tr><td style="background-color:#fff;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:middle;word-break:normal" rowspan="6">Axt-Sum</td><td style="background-color:#fff;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal">L2L^{cov}_{ptr}</td><td style="background-color:#fff;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal"><a href="https://arxiv.org/pdf/1704.04368.pdf" target="_blank" rel="noopener noreferrer">Get to the   point: Summarization with Pointer-Generator Networks</a></td><td style="background-color:#fff;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal"><a href="https://www.aclweb.org/anthology/P17-1099.bib" target="_blank" rel="noopener noreferrer">Bib</a></td></tr><tr><td style="background-color:#f9f9f9;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal">L2L_{ptr}</td><td style="background-color:#f9f9f9;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal"><a href="https://arxiv.org/pdf/1704.04368.pdf" target="_blank" rel="noopener noreferrer">Get to the   point: Summarization withpointer-generator networks</a></td><td style="background-color:#f9f9f9;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal"><a href="https://www.aclweb.org/anthology/P17-1099.bib" target="_blank" rel="noopener noreferrer">Bib</a></td></tr><tr><td style="background-color:#fff;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal">L2L</td><td style="background-color:#fff;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal"><a href="https://github.com/zide05/CDEvalSumm" target="_blank" rel="noopener noreferrer">CDEvalSumm: An   Empirical Study of Cross-Dataset Evaluationfor Neural Summarization   Systems</a></td><td style="background-color:#fff;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal"><a href="https://github.com/zide05/CDEvalSumm" target="_blank" rel="noopener noreferrer">Bib</a></td></tr><tr><td style="background-color:#f9f9f9;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal">T2T</td><td style="background-color:#f9f9f9;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal"><a href="https://arxiv.org/pdf/1908.08345.pdf" target="_blank" rel="noopener noreferrer">Text   Summarization  with  Pretrained    Encoders</a></td><td style="background-color:#f9f9f9;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal"><a href="https://www.aclweb.org/anthology/D19-1387.bib" target="_blank" rel="noopener noreferrer">Bib</a></td></tr><tr><td style="background-color:#fff;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal">BE2T</td><td style="background-color:#fff;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal"><a href="https://arxiv.org/pdf/1908.08345.pdf" target="_blank" rel="noopener noreferrer">Text   Summarization  with  Pretrained    Encoders</a></td><td style="background-color:#fff;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal"><a href="https://www.aclweb.org/anthology/D19-1387.bib" target="_blank" rel="noopener noreferrer">Bib</a></td></tr><tr><td style="background-color:#f9f9f9;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal">BART</td><td style="background-color:#f9f9f9;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal"><a href="https://arxiv.org/pdf/1810.12343.pdf" target="_blank" rel="noopener noreferrer">Bart:  Denoising    Sequence-to-Sequence    Pre-training for  Natural  Language    Generation,  Translation,  and Comprehension</a></td><td style="background-color:#f9f9f9;border-color:inherit;border-style:solid;border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:bottom;word-break:normal"><a href="https://www.aclweb.org/anthology/2020.acl-main.703.bib" target="_blank" rel="noopener noreferrer">Bib</a></td></tr></tbody></table>





## Datasets

<img src="https://github.com/zide05/CompSUM/blob/master/figs/spider.png" width="750" height="250">

+ Datasets
  - [CNN/Dailymail](https://arxiv.org/pdf/1602.06023.pdf)
  - [Xsum](https://arxiv.org/pdf/1808.08745.pdf)
  - [Pubmed](https://arxiv.org/pdf/1804.05685.pdf)
  - [Bigpatent B](https://arxiv.org/pdf/1906.03741.pdf)
  - [Reddit TIFU](https://arxiv.org/pdf/1811.00783.pdf)

   

## Evaluation Metrics
+ Semantic Equivalenc (ROUGE)
+ Factuality ([Factcc](https://arxiv.org/pdf/1910.12840.pdf)) 
+ Dataset bias (Detailed explanation is displayed in our paper and the code can refer to Data-bias-metrics/)
  + [Coverage](https://arxiv.org/pdf/1804.11283.pdf) 
  + Copy length
  + [Novelty](https://arxiv.org/pdf/1704.04368.pdf)
  + [Repetition](https://arxiv.org/pdf/1704.04368.pdf)
  + [Sentence fusion score](https://arxiv.org/pdf/1906.00077.pdf) 
  

## Cross-dataset Measures
+ Stiffness 
  <br><br>
  <img src="https://render.githubusercontent.com/render/math?math=r^{\mu} = \frac{1}{N\times N}\sum_{i,j} {\mathbf{U}}_{ij}"> <br>
  <img src="https://render.githubusercontent.com/render/math?math={\mathbf{U}}_{ij}"> : the metric score when model is trained on dataset i and tested on dataset j.
+ Stableness  
  <br>
  <img src="https://render.githubusercontent.com/render/math?math=r^{\sigma} = \frac{1}{N\times N}\sum_{i,j} \mathbf{U}_{ij}/ \mathbf{U}_{jj}\times100 \%25"> <br>
  <img src="https://render.githubusercontent.com/render/math?math={\mathbf{U}}_{ij}"> : the metric score when model is trained on dataset i and tested on dataset j.

## Experiment Results
The stiffness and stableness of various summarizers are displayed below. For fine-grained results and comprehensive analysis please refer to the paper.
<div align="center"><img src="https://github.com/zide05/CompSUM/blob/master/figs/rouge_all.PNG" width="310" height="330">&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<img src="https://github.com/zide05/CompSUM/blob/master/figs/factcc_all.PNG" width="320" height="330"></div>


